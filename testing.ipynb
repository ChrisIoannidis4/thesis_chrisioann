{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5671, 3)\n",
      "(2000, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import numpy as np\n",
    "def select_subset(array, subset_size):\n",
    "    subset_indices = random.sample(range(len(array)), subset_size)\n",
    "    subset = [array[i] for i in subset_indices]\n",
    "    return np.array(subset)\n",
    "origial_array=np.load('data/temp/sampled_coordinates.npy')\n",
    "print(origial_array.shape)\n",
    "coord_subset = select_subset(origial_array, 2000)\n",
    "print(coord_subset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 438)\n"
     ]
    }
   ],
   "source": [
    "from functions.c_optimization_problem import initialize_dictionary\n",
    "import numpy as np\n",
    "\n",
    "Weight_Space = np.load(\"data/X_W_arrays/W_space.npy\")\n",
    "\n",
    "print(Weight_Space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dw = initialize_dictionary(Weight_Space, 20)\n",
    "print(Dw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 20)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def initialize_coefficients(D, all_ellements, a):\n",
    "    #X: (150, 500), d: (150X25), a: soothing factor\n",
    "    coefficients=np.zeros((D.shape[1] , all_ellements.shape[1]))\n",
    "    \n",
    "    for j, sample in enumerate(all_ellements.T):\n",
    "        for i, word in enumerate(D.T):\n",
    "            diff = np.linalg.norm(sample-word)**2\n",
    "            c_1=np.exp(-a * diff)\n",
    "            coefficients[i][j]=c_1\n",
    "    coefficients[:,j]/=np.sum(coefficients[:,j])\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.c_optimization_problem import initialize_coefficients\n",
    "\n",
    "Lw=initialize_coefficients(Dw, Weight_Space, 0.2)\n",
    "print(Lw.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/dictionaries/Dw\", Weight_Space)\n",
    "np.save(\"data/dictionaries/Lw\", Lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1] (384, 384, 160)\n",
      "(20481580, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_roi(subj_no):\n",
    "    roi = np.load(\"data/MRI_MASKS/roi_masks_dataset/roi_\"+ subj_no + \".npy\")\n",
    "    return roi \n",
    "\n",
    "roi = load_roi(\"9002430\")\n",
    "print(np.unique(roi), roi.shape)\n",
    "\n",
    "\n",
    "def gather_roi_indices(roi_array):\n",
    "    roi_inices=np.array(np.where(roi_array == 1)).T\n",
    "    # print(roi_inices)\n",
    "    roi_perimeter= [np.array(coord) for coord in roi_inices]\n",
    "    return np.array(roi_perimeter)\n",
    "# roi_perimeter=gather_roi_indices(roi)\n",
    "# print(roi_perimeter.shape, roi_perimeter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_perimeter = []\n",
    "for x in range(88,302,6):\n",
    "    for y in range(63,321,6):\n",
    "        for z in range(12,147,6):\n",
    "            roi_perimeter.append([x,y,z])\n",
    "\n",
    "roi_perimeter = np.array(roi_perimeter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MySoftware\\python3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=120, random_state=1, verbose=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=120, random_state=1, verbose=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=120, random_state=1, verbose=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_state=1\n",
    "kmeans_model1=KMeans(n_clusters=120, verbose=False, init='k-means++', random_state=random_state)\n",
    "kmeans_model1.fit(roi_perimeter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MySoftware\\python3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(n_clusters=5, random_state=1, verbose=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(n_clusters=5, random_state=1, verbose=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(n_clusters=5, random_state=1, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model2=KMeans(n_clusters=5, verbose=False, init='k-means++', random_state=random_state)\n",
    "kmeans_model2.fit(kmeans_model1.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[143.21343437, 284.26397253,  78.97700227],\n",
       "       [249.42306288, 130.42189711,  78.86156424],\n",
       "       [141.64832759, 113.13232281,  76.02412297],\n",
       "       [250.02234419, 257.040163  ,  79.61890683],\n",
       "       [148.85776705, 202.2834326 ,  82.29225956]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_model2.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60372, 3)\n"
     ]
    }
   ],
   "source": [
    "roi_subset = []\n",
    "subregion_centers=kmeans_model2.cluster_centers_\n",
    "for coordinate in roi_perimeter:\n",
    "    x,y,z=coordinate\n",
    "    if x%5==0 and y%5==0 and z%5==0:\n",
    "        roi_subset.append(coordinate)\n",
    "roi_subset=np.array(roi_subset)\n",
    "print(roi_subset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_label = kmeans_model1.predict(roi_subset)\n",
    "final_label = kmeans_model2.predict(kmeans_model1.cluster_centers_[first_label])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "subregion_1 = roi_subset[final_label==0]\n",
    "subregion_2 = roi_subset[final_label==1]\n",
    "subregion_3 = roi_subset[final_label==2]\n",
    "subregion_4 = roi_subset[final_label==3]\n",
    "subregion_5 = roi_subset[final_label==4]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9561, 3)  --  (15142, 3)  --  (11851, 3)  --  (13755, 3)  --  (10063, 3)\n"
     ]
    }
   ],
   "source": [
    "print(subregion_1.shape, \" -- \", subregion_2.shape, \" -- \",subregion_3.shape, \" -- \",subregion_4.shape, \" -- \",subregion_5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"coordinates/sub_1_coord.npy\", subregion_1)\n",
    "np.save(\"coordinates/sub_2_coord.npy\", subregion_2)\n",
    "np.save(\"coordinates/sub_3_coord.npy\", subregion_3)\n",
    "np.save(\"coordinates/sub_4_coord.npy\", subregion_4)\n",
    "np.save(\"coordinates/sub_5_coord.npy\", subregion_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates= np.load(\"data/temp/general_coordinates.npy\")\n",
    "dir1 = os.listdir(\"data/MRI_MASKS/subjects\")\n",
    "dir2 = [(os.listdir(\"data/MRI_MASKS/segmentation_masks\")[i]).split(\".\")[0] for i in range(len(os.listdir(\"data/MRI_MASKS/segmentation_masks\")))]\n",
    "dir3 = [(os.listdir(\"data/MRI_MASKS/roi_masks_dataset\")[i]).split(\".\")[0][4:] for i in range(len(os.listdir(\"data/MRI_MASKS/roi_masks_dataset\")))]\n",
    "common_subjects = list(set(dir1).intersection(dir2).intersection(dir3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 437) (640, 438)\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"data/X_W_arrays/X_space.npy\")\n",
    "W = np.load(\"data/X_W_arrays/W_space.npy\")\n",
    "\n",
    "print(X.shape, W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir1 = os.listdir(\"data/MRI_MASKS/subjects\")\n",
    "dir2 = [(os.listdir(\"data/MRI_MASKS/segmentation_masks\")[i]).split(\".\")[0] for i in range(len(os.listdir(\"data/MRI_MASKS/segmentation_masks\")))]\n",
    "common_subjects = list(set(dir1).intersection(dir2))\n",
    "print(len(common_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n"
     ]
    }
   ],
   "source": [
    "dir1 = os.listdir(\"data/MRI_MASKS/subjects\")\n",
    "dir2 = [(os.listdir(\"data/MRI_MASKS/segmentation_masks\")[i]).split(\".\")[0] for i in range(len(os.listdir(\"data/MRI_MASKS/segmentation_masks\")))]\n",
    "common_subjects = list(set(dir1).intersection(dir2))\n",
    "print(len(common_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 25)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(np.load(\"data/dictionaries/Dw.npy\").shape)\n",
    "Dx = initialize_dictionary(X, 25)\n",
    "print(Dx.shape)\n",
    "np.save(\"data/dictionaries/Dx.npy\",Dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lx = initialize_coefficients(Dx, X, 0.8) #(25, 437)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/dictionaries/Lx.npy\", Lx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functions.c_optimization_problem import *\n",
    "\n",
    "W = np.load(\"data/X_W_arrays/W_space.npy\")\n",
    "Dw = initialize_dictionary(W, 25)\n",
    "Lw = initialize_coefficients(Dw, W, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"data/dictionaries/W.npy\", W)\n",
    "np.save(\"data/dictionaries/Dw.npy\", Dw)\n",
    "np.save(\"data/dictionaries/LW.npy\", Lw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.07317933345089 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "roi1=np.load('data/MRI_MASKS/roi_masks_dataset/roi_9001104.npy')\n",
    "roi2=np.load('data/MRI_MASKS/roi_masks_dataset/roi_9023193.npy')\n",
    "roi3=np.load('data/MRI_MASKS/roi_masks_dataset/roi_9011420.npy')\n",
    "\n",
    "coord=np.load('data/temp/sampled_coordinates.npy')\n",
    "\n",
    "i=0\n",
    "\n",
    "for coordinate in coord:\n",
    "    x,y,z = coordinate\n",
    "    if roi2[x,y,z] ==roi1 [x,y,z] == roi3 [x,y,z]:\n",
    "        i+=1\n",
    "\n",
    "print( i*100 / len(coord) , \"%\" )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING QCQP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dictionary(all_elements, Dictionary_Size):\n",
    "    \n",
    "    k = Dictionary_Size\n",
    "    descr_kmeans=KMeans(n_clusters=k, random_state=0)\n",
    "    descr_kmeans.fit(all_elements.T)\n",
    "    dictionary = descr_kmeans.cluster_centers_  \n",
    "\n",
    "    return dictionary.T\n",
    "\n",
    "#Soft voting to initialize Lx (Zhang et al 2013 - 2.Soft Assignment Coding)\n",
    "def initialize_coefficients(D, all_ellements, a):\n",
    "    #X: (150, 500), d: (150X25), a: soothing factor\n",
    "    coefficients=np.zeros((D.shape[1] , all_ellements.shape[1]))\n",
    "    \n",
    "    for j, sample in enumerate(all_ellements.T):\n",
    "        for i, word in enumerate(D.T):\n",
    "            diff = np.linalg.norm(sample-word)**2 \n",
    "            c_1=np.exp(-a * diff)\n",
    "            coefficients[i][j]=c_1\n",
    "    coefficients[:,j]/=np.sum(coefficients[:,j])\n",
    "    return coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MySoftware\\python3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize#, NonlinearConstraint \n",
    "from sklearn.cluster import KMeans \n",
    "# Objective function\n",
    "def objective(dx, X, Lx):\n",
    "    dx =dx.reshape(dx.shape[0],1)\n",
    "    return np.linalg.norm(X - dx @ Lx, ord='fro')#**2\n",
    "\n",
    "# Constraint function\n",
    "def constraint(dx):\n",
    "    return 1 - np.linalg.norm(dx, ord=2)\n",
    "\n",
    "def constraint_jacobian(dx):\n",
    "    return - (dx / np.linalg.norm(dx, ord=2))\n",
    "\n",
    "X = np.random.rand(150, 500)\n",
    "Dx_init = initialize_dictionary(X, 25)\n",
    "Lx = initialize_coefficients(Dx_init, X, 0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChrisIoannidis\\AppData\\Local\\Temp\\ipykernel_11496\\2448636275.py:18: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  result = minimize(objective, dx_init, args=(A, B), method= 'COBYLA', constraints=[constraint_definition],jac = constraint_jacobian ,options={'maxiter' : 1000 , 'disp' : False })\n",
      "c:\\MySoftware\\python3\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:549: RuntimeWarning: Method COBYLA does not use gradient information (jac).\n",
      "  warn('Method %s does not use gradient information (jac).' % method,\n"
     ]
    }
   ],
   "source": [
    "n = X.shape[1]\n",
    "        \n",
    "        \n",
    "        # Constraint definition\n",
    "constraint_definition = {'type': 'ineq', 'fun': constraint}     \n",
    "\n",
    "def col_by_col_step(Dx_init):\n",
    "\n",
    "    Dx = Dx_init.copy()\n",
    "    \n",
    "    for i, dx_init in enumerate(Dx_init.T):\n",
    "        # Bounds for dx\n",
    "        dx_init = np.reshape(dx_init, [dx_init.shape[0], 1])\n",
    "        # constraint_definition = NonlinearConstraint(constraint, -np.inf, 1, jac=constraint_jacobian)\n",
    "        # objective =  cp.Minimize(cp.sum_squares( + Di@L[i,:].reshape(1,no_samples)))\n",
    "        A = X - np.sum([Dx[:,j].reshape(Dx[:,j].shape[0],1)@Lx[j,:].reshape(1,n) for j in range(Dx.shape[1]) if j!=i], axis=0)\n",
    "        B = Lx[i, :].reshape(1,n)\n",
    "        result = minimize(objective, dx_init, args=(A, B), method= 'COBYLA', constraints=[constraint_definition], options={'maxiter' : 1000 , 'disp' : False })\n",
    "\n",
    "        # Obtained solution\n",
    "        dx_optimal = result.x\n",
    "        \n",
    "        # print(np.linalg.norm(dx_optimal))\n",
    "        Dx[:,i] = dx_optimal\n",
    "\n",
    "    return Dx\n",
    "\n",
    "Dx = col_by_col_step(Dx_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 157.93763276395114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ChrisIoannidis\\AppData\\Local\\Temp\\ipykernel_11496\\2448636275.py:18: DeprecationWarning: Use of `minimize` with `x0.ndim != 1` is deprecated. Currently, singleton dimensions will be removed from `x0`, but an error will be raised in SciPy 1.11.0.\n",
      "  result = minimize(objective, dx_init, args=(A, B), method= 'COBYLA', constraints=[constraint_definition],jac = constraint_jacobian ,options={'maxiter' : 1000 , 'disp' : False })\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157.93763276395114\n",
      "157.95099537042518\n",
      "157.98538950860853\n",
      "157.982432517066\n",
      "157.98440889237486\n",
      "157.99306041789603\n",
      "157.9745768613824\n",
      "157.98632085643854\n",
      "157.98073586379698\n",
      "157.98531975626346\n"
     ]
    }
   ],
   "source": [
    "def optimization_loop(Dx_init):\n",
    "    norms =[]\n",
    "    Dx = Dx_init.copy()\n",
    "    for i in range(100): \n",
    "        Dx = col_by_col_step(Dx)\n",
    "        print(np.linalg.norm(X - Dx @ Lx, ord='fro'))\n",
    "    return Dx\n",
    "\n",
    "print('before' , np.linalg.norm(X - Dx @ Lx, ord='fro'))\n",
    "\n",
    "\n",
    "Dx = optimization_loop(Dx_init)\n",
    "\n",
    "\n",
    "print('after' , np.linalg.norm(X - Dx @ Lx, ord='fro'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cvx\n",
    "\n",
    "print(cvx.__version__)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_dictionaries_step(X, D, L, n, no_samples):\n",
    "    k = 25\n",
    "    # (150x500) = (150x1)(1x500)\n",
    "    for i in range(k):    \n",
    "        Di = cp.Variable((n,1))\n",
    "        # print(\"old: \",D[:10,i])\n",
    "        objective =  cp.Minimize(cp.sum_squares(X - np.sum([D[:,j]@L[j,:].reshape(1,no_samples) \n",
    "                                                           for j in range(k) if i!=i]) + Di@L[i,:].reshape(1,no_samples)))\n",
    "                                                 # cp.Minimize(np.linalg.norm(X - Dx@Lx, \"fro\"))\n",
    "        \n",
    "        constraints = [cp.norm2(Di) <= 1]\n",
    "\n",
    "        prob = cp.Problem(objective, constraints) \n",
    "        # try:\n",
    "        prob.solve(solver='OSQP',verbose=True)\n",
    "        # except cp.error.SolverError:\n",
    "        #     prob.solve(solver='SCS')\n",
    "\n",
    "        print(prob.status) #infea/sible_inaccurate\n",
    "        D[:,i] = Di.value.reshape(n,) #AttributeError: 'NoneType' object has no attribute 'reshape'\n",
    "\n",
    "    return D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MySoftware\\python3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X = np.random.rand(150, 500)\n",
    "Dx_init = initialize_dictionary(X, 25)\n",
    "Lx = initialize_coefficients(Dx_init, X, 0.8)\n",
    "n = 150\n",
    "no_samples=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.3.1                                    \n",
      "===============================================================================\n",
      "(CVXPY) Jun 23 01:48:17 AM: Your problem has 150 variables, 1 constraints, and 0 parameters.\n",
      "(CVXPY) Jun 23 01:48:17 AM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Jun 23 01:48:17 AM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Jun 23 01:48:17 AM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n"
     ]
    },
    {
     "ename": "SolverError",
     "evalue": "Problem could not be reduced to a QP, and no conic solvers exist among candidate solvers ({'qp_solvers': ['OSQP'], 'conic_solvers': []}).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSolverError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m D_new \u001b[39m=\u001b[39m update_dictionaries_step(X, Dx_init, Lx, n, no_samples)\n",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m, in \u001b[0;36mupdate_dictionaries_step\u001b[1;34m(X, D, L, n, no_samples)\u001b[0m\n\u001b[0;32m     13\u001b[0m prob \u001b[39m=\u001b[39m cp\u001b[39m.\u001b[39mProblem(objective, constraints) \n\u001b[0;32m     14\u001b[0m \u001b[39m# try:\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m prob\u001b[39m.\u001b[39;49msolve(solver\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mOSQP\u001b[39;49m\u001b[39m'\u001b[39;49m,verbose\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     16\u001b[0m \u001b[39m# except cp.error.SolverError:\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m#     prob.solve(solver='SCS')\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(prob\u001b[39m.\u001b[39mstatus) \u001b[39m#infea/sible_inaccurate\u001b[39;00m\n",
      "File \u001b[1;32mc:\\MySoftware\\python3\\Lib\\site-packages\\cvxpy\\problems\\problem.py:493\u001b[0m, in \u001b[0;36mProblem.solve\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    492\u001b[0m     solve_func \u001b[39m=\u001b[39m Problem\u001b[39m.\u001b[39m_solve\n\u001b[1;32m--> 493\u001b[0m \u001b[39mreturn\u001b[39;00m solve_func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\MySoftware\\python3\\Lib\\site-packages\\cvxpy\\problems\\problem.py:1054\u001b[0m, in \u001b[0;36mProblem._solve\u001b[1;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, ignore_dpp, canon_backend, **kwargs)\u001b[0m\n\u001b[0;32m   1051\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munpack(chain\u001b[39m.\u001b[39mretrieve(soln))\n\u001b[0;32m   1052\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue\n\u001b[1;32m-> 1054\u001b[0m data, solving_chain, inverse_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_problem_data(\n\u001b[0;32m   1055\u001b[0m     solver, gp, enforce_dpp, ignore_dpp, verbose, canon_backend, kwargs\n\u001b[0;32m   1056\u001b[0m )\n\u001b[0;32m   1058\u001b[0m \u001b[39mif\u001b[39;00m verbose:\n\u001b[0;32m   1059\u001b[0m     \u001b[39mprint\u001b[39m(_NUM_SOLVER_STR)\n",
      "File \u001b[1;32mc:\\MySoftware\\python3\\Lib\\site-packages\\cvxpy\\problems\\problem.py:631\u001b[0m, in \u001b[0;36mProblem.get_problem_data\u001b[1;34m(self, solver, gp, enforce_dpp, ignore_dpp, verbose, canon_backend, solver_opts)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mkey:\n\u001b[0;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39minvalidate()\n\u001b[1;32m--> 631\u001b[0m     solving_chain \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_construct_chain(\n\u001b[0;32m    632\u001b[0m         solver\u001b[39m=\u001b[39;49msolver, gp\u001b[39m=\u001b[39;49mgp,\n\u001b[0;32m    633\u001b[0m         enforce_dpp\u001b[39m=\u001b[39;49menforce_dpp,\n\u001b[0;32m    634\u001b[0m         ignore_dpp\u001b[39m=\u001b[39;49mignore_dpp,\n\u001b[0;32m    635\u001b[0m         canon_backend\u001b[39m=\u001b[39;49mcanon_backend,\n\u001b[0;32m    636\u001b[0m         solver_opts\u001b[39m=\u001b[39;49msolver_opts)\n\u001b[0;32m    637\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39mkey \u001b[39m=\u001b[39m key\n\u001b[0;32m    638\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cache\u001b[39m.\u001b[39msolving_chain \u001b[39m=\u001b[39m solving_chain\n",
      "File \u001b[1;32mc:\\MySoftware\\python3\\Lib\\site-packages\\cvxpy\\problems\\problem.py:883\u001b[0m, in \u001b[0;36mProblem._construct_chain\u001b[1;34m(self, solver, gp, enforce_dpp, ignore_dpp, canon_backend, solver_opts)\u001b[0m\n\u001b[0;32m    881\u001b[0m candidate_solvers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_candidate_solvers(solver\u001b[39m=\u001b[39msolver, gp\u001b[39m=\u001b[39mgp)\n\u001b[0;32m    882\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_candidate_solvers(candidate_solvers)\n\u001b[1;32m--> 883\u001b[0m \u001b[39mreturn\u001b[39;00m construct_solving_chain(\u001b[39mself\u001b[39;49m, candidate_solvers, gp\u001b[39m=\u001b[39;49mgp,\n\u001b[0;32m    884\u001b[0m                                enforce_dpp\u001b[39m=\u001b[39;49menforce_dpp,\n\u001b[0;32m    885\u001b[0m                                ignore_dpp\u001b[39m=\u001b[39;49mignore_dpp,\n\u001b[0;32m    886\u001b[0m                                canon_backend\u001b[39m=\u001b[39;49mcanon_backend,\n\u001b[0;32m    887\u001b[0m                                solver_opts\u001b[39m=\u001b[39;49msolver_opts)\n",
      "File \u001b[1;32mc:\\MySoftware\\python3\\Lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:184\u001b[0m, in \u001b[0;36mconstruct_solving_chain\u001b[1;34m(problem, candidates, gp, enforce_dpp, ignore_dpp, canon_backend, solver_opts)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(problem\u001b[39m.\u001b[39mvariables()) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m SolvingChain(reductions\u001b[39m=\u001b[39m[ConstantSolver()])\n\u001b[1;32m--> 184\u001b[0m reductions \u001b[39m=\u001b[39m _reductions_for_problem_class(problem, candidates, gp, solver_opts)\n\u001b[0;32m    186\u001b[0m \u001b[39m# Process DPP status of the problem.\u001b[39;00m\n\u001b[0;32m    187\u001b[0m dpp_context \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdcp\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m gp \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdgp\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\MySoftware\\python3\\Lib\\site-packages\\cvxpy\\reductions\\solvers\\solving_chain.py:124\u001b[0m, in \u001b[0;36m_reductions_for_problem_class\u001b[1;34m(problem, candidates, gp, solver_opts)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     \u001b[39m# Canonicalize it to conic problem.\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m candidates[\u001b[39m'\u001b[39m\u001b[39mconic_solvers\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m--> 124\u001b[0m         \u001b[39mraise\u001b[39;00m SolverError(\u001b[39m\"\u001b[39m\u001b[39mProblem could not be reduced to a QP, and no \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39mconic solvers exist among candidate solvers \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    126\u001b[0m                           \u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m candidates)\n\u001b[0;32m    128\u001b[0m constr_types \u001b[39m=\u001b[39m {\u001b[39mtype\u001b[39m(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m problem\u001b[39m.\u001b[39mconstraints}\n\u001b[0;32m    129\u001b[0m \u001b[39mif\u001b[39;00m FiniteSet \u001b[39min\u001b[39;00m constr_types:\n",
      "\u001b[1;31mSolverError\u001b[0m: Problem could not be reduced to a QP, and no conic solvers exist among candidate solvers ({'qp_solvers': ['OSQP'], 'conic_solvers': []})."
     ]
    }
   ],
   "source": [
    "D_new = update_dictionaries_step(X, Dx_init, Lx, n, no_samples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MySoftware\\python3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans \n",
    "\n",
    "W = np.random.uniform(0,1, [645, 500])\n",
    "Dw = initialize_dictionary(W, 25)\n",
    "Lx = np.random.uniform(0,1, [25, 500])\n",
    "M = np.random.uniform(0,1, [25, 25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.linear_model import LassoLars\n",
    " \n",
    "\n",
    "def lars_col_by_col(X, y , alpha=0.01):\n",
    "    lasso = LassoLars(alpha=alpha)\n",
    "    lasso.fit(X, y)\n",
    "    return lasso.coef_\n",
    "\n",
    "\n",
    "def update_Lw_step(W, Dw, M, Lx, gamma=0.7, alpha=0.01):\n",
    "\n",
    "    Y = np.vstack([W, gamma * M @ Lx])\n",
    "    X = np.vstack([Dw, gamma * np.identity(25)])\n",
    "    Lw = np.zeros([Dw.shape[1], W.shape[1]])\n",
    "    \n",
    "    for i, y in enumerate(Y.T):\n",
    "    \n",
    "        lw_col = lars_col_by_col(X, y)\n",
    "        Lw[:, i] = lw_col\n",
    "\n",
    "    return Lw\n",
    "\n",
    "Lw = update_Lw_step(W, Dw, M, Lx, gamma=0.7, alpha=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(870966, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "coordinate_cube = np.load('coordinates/coordinate_cube.npy')\n",
    "print(coordinate_cube.shape)\n",
    "# coordinates/coordinate_cube.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lists_of_of_paths(directory):\n",
    "    file_list = os.listdir(directory)\n",
    "    file_paths = []\n",
    "    for file in file_list:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        file_paths.append(file_path)\n",
    "\n",
    "    return file_paths  \n",
    "\n",
    "def svm_coordinate_sampling(segm, roi):\n",
    "    coord_labels = []\n",
    "    coordinates = []\n",
    "    \n",
    "    mask_values = segm[coordinate_cube[:,0], coordinate_cube[:,1], coordinate_cube[:,2]]\n",
    "\n",
    "    threshold_4 = np.count_nonzero(mask_values == 4) #.shape[0]#len(segm[segm[coordinate_cube]==4])\n",
    "    \n",
    "    threshold_1=int(threshold_4/1.2)\n",
    "    threshold_2=int(threshold_4*1.3)\n",
    "    threshold_3=int(threshold_4/1.1)\n",
    "    threshold_0=int(threshold_4/1.3)\n",
    "   \n",
    "    i_0,i_1,i_2,i_3,i_4 =0, 0, 0, 0, 0\n",
    "    \n",
    "    print(threshold_0,threshold_1,threshold_2,threshold_3,threshold_4)\n",
    "        \n",
    "        \n",
    "    for coordinate in coordinate_cube:\n",
    "        x,y,z = coordinate\n",
    "        if roi[x,y,z]==1 and 360>x>6 and 360>y>6 and 150>z>6:\n",
    "            if segm[x,y,z]==0 and i_0<threshold_0:\n",
    "                coordinates.append(coordinate)\n",
    "                i_0+=1\n",
    "                coord_labels.append(0)\n",
    "            elif segm[x,y,z]==1 and i_1<threshold_1:\n",
    "                coordinates.append(coordinate)\n",
    "                i_1+=1\n",
    "                coord_labels.append(1)\n",
    "            elif segm[x,y,z]==2 and i_2<threshold_2:\n",
    "                coordinates.append(coordinate)\n",
    "                i_2+=1\n",
    "                coord_labels.append(2)\n",
    "            elif segm[x,y,z]==3 and i_3<threshold_3:\n",
    "                coordinates.append(coordinate)\n",
    "                i_3+=1\n",
    "                coord_labels.append(3)\n",
    "            elif segm[x,y,z]==4 and i_4<threshold_4:\n",
    "                coordinates.append(coordinate)\n",
    "                i_4+=1 \n",
    "                coord_labels.append(4)\n",
    "                \n",
    "    coord_labels=np.array(coord_labels)\n",
    "    unique_values, counts = np.unique(coord_labels, return_counts=True)\n",
    "\n",
    "    for value, count in zip(unique_values, counts):\n",
    "        print(f\"{value}: {count}\")\n",
    "    coordinates = np.array(coordinates)  \n",
    "    np.random.shuffle(coordinates)      \n",
    "    return coordinates\n",
    "\n",
    "\n",
    "def fn_scan_to_array(base_path):\n",
    "    mid_file=get_lists_of_of_paths(base_path)[0]\n",
    "    slice_list=get_lists_of_of_paths(mid_file)\n",
    "    for i in range(160):\n",
    "        if i==0:    \n",
    "            slice = sitk.ReadImage(get_lists_of_of_paths(mid_file)[0])\n",
    "            scan_array3D = (sitk.GetArrayFromImage(slice)).reshape(384,384)\n",
    "        else:  \n",
    "            slice = sitk.ReadImage(get_lists_of_of_paths(mid_file)[i])\n",
    "            slice_array= sitk.GetArrayFromImage(slice).reshape(384,384)\n",
    "            scan_array3D=np.dstack((scan_array3D, slice_array))\n",
    "    return scan_array3D  # (384, 384, 160)\n",
    "\n",
    "\n",
    "\n",
    "def fn_segm_mask_to_array(subject_name):\n",
    "\n",
    "    mhd_path = \"data/\" + subject_name +\"/\"+subject_name+\".segmentation_masks.mhd\"\n",
    "    segm_mask = sitk.GetArrayFromImage(sitk.ReadImage(mhd_path, sitk.sitkFloat32))\n",
    "    return np.flip(segm_mask, axis=0) # (384, 384, 160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3369 2356 3369 2356 2592\n",
      "0: 3369\n",
      "1: 2356\n",
      "2: 3369\n",
      "3: 2356\n",
      "4: 2592\n",
      "(14042, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "mri_scan = fn_scan_to_array('data/9004175')\n",
    "segm_mask = fn_segm_mask_to_array('9004175')\n",
    "roi = np.load('data/9004175/roi.npy')\n",
    "coordinates = svm_coordinate_sampling(segm_mask, roi)\n",
    "\n",
    "print(coordinates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192,)\n",
      "created datasets\n",
      "(11233, 192) (11233,)\n",
      "fitting model\n",
      "Accuracy: 0.8166607333570666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Step 3: Prepare the data for SVM training\n",
    "descriptors = []\n",
    "labels = []\n",
    "for coord in coordinates:\n",
    "    descriptor = fn_hog_lbp_descriptor(coord, mri_scan)\n",
    "    # if descriptor!= None # makeshiftSIFT(mri_scan, coord)\n",
    "    descriptor = np.reshape(descriptor, [descriptor.shape[1],])\n",
    "    descriptors.append(descriptor)\n",
    "    value = segm_mask[coord[0], coord[1], coord[2]]\n",
    "    if value == 0:\n",
    "        labels.append(0)\n",
    "    elif value == 1 or value == 3:\n",
    "        labels.append(1)\n",
    "    elif value == 2 or value == 4:\n",
    "        labels.append(2)\n",
    "    else:\n",
    "        labels.append(value)   \n",
    "    \n",
    "    if len(descriptors) == 1:\n",
    "        print(descriptor.shape)\n",
    "    # labels.append(value)\n",
    "    \n",
    "    \n",
    "X = np.array(descriptors)\n",
    "y = np.array(labels)\n",
    "\n",
    "print('created datasets')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape )\n",
    "# Step 4: Train the SVM\n",
    "svm = SVC(kernel= 'linear', decision_function_shape ='ovr')\n",
    "print('fitting model')\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Predict on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          3.09487747  0.         ...  2.28650718  0.77724042\n",
      "  -0.06492689]\n",
      " [ 0.          6.48209861  0.         ...  2.81969127  2.03089751\n",
      "   1.48111708]\n",
      " [ 0.         -0.45139017  0.         ...  1.81179989 -0.21771726\n",
      "  -1.47661083]\n",
      " ...\n",
      " [ 0.         -7.36939677  0.         ... -0.94874272 -2.8350578\n",
      "  -2.75561552]\n",
      " [ 0.         -4.11394554  0.         ...  1.00016369  1.11563795\n",
      "   0.09218526]\n",
      " [ 0.          2.27654165  0.         ...  2.46991985  3.72289172\n",
      "   2.7139123 ]] [  5.0645478  -24.52334751  11.59786284  -9.76290759 -30.11749378\n",
      "  14.74065303 -15.64361735  27.79625565   7.30623019 -23.29556783]\n",
      "true 3.0\n",
      "pred_1 [3.]\n",
      "Predicted class: 4\n"
     ]
    }
   ],
   "source": [
    "print(svm.coef_, svm.intercept_)\n",
    "\n",
    "coord = coordinates[100]\n",
    "descriptor = fn_hog_lbp_descriptor(coord, mri_scan)\n",
    "\n",
    "\n",
    "value = segm_mask[coord[0], coord[1], coord[2]]\n",
    "\n",
    "print(\"true\", value)\n",
    "print(\"pred_1\", svm.predict(descriptor))\n",
    "if len(descriptor.shape) == 1:\n",
    "    descriptor = descriptor.reshape(1, -1)\n",
    "decision_value = descriptor.dot(svm.coef_.T) + svm.intercept_\n",
    "predicted_class = decision_value.argmax()\n",
    "\n",
    "print(\"Predicted class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.72      0.74       670\n",
      "           1       0.83      0.88      0.85       959\n",
      "           2       0.84      0.82      0.83      1180\n",
      "\n",
      "    accuracy                           0.82      2809\n",
      "   macro avg       0.81      0.81      0.81      2809\n",
      "weighted avg       0.82      0.82      0.82      2809\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbp_ri_sh(point, img, patch_size, sph_degree, ico_radius, ico_level, n_bins = 30, concatenate = True):\n",
    "    '''\n",
    "    Computes a 3D LBP texture descriptor for a region centered around a voxel. The intensity values \n",
    "    of the neighboring voxels is treated as a spherical function, and decomposed into a sum of\n",
    "    spherical harmonics, achieving rotational invariance. A histogram of the texture codes is computed for\n",
    "    each frequency (band) and the final descriptor is the concatenation of the above histograms.\n",
    "\n",
    "    Reference : 3D LBP-Based Rotationally Invariant Region Description, \n",
    "                Banerjee J., Moelker A., Niessen W., Walsum  v. T., \n",
    "                ACCV 2012 Workshops, Part I, LNCS 7728, pp. 26-37, 2013\n",
    "\n",
    "    ...\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point : tuple\n",
    "        the point to be described\n",
    "    img : ndarray (width x height x depth)\n",
    "        the MR image\n",
    "    patch_size : int\n",
    "        size of cellular patch around point\n",
    "    sph_degree : int\n",
    "        degree up to which to expand to spherical harmonics\n",
    "    ico_radius : float\n",
    "        radius of icosahedron to uniformly sample intensities around patch voxels\n",
    "    ico_level : int\n",
    "        controls refinement level of icosahedron\n",
    "    n_bins : int\n",
    "        number of bins for LBP histograms\n",
    "    concatenate : bool\n",
    "        if true, concatenate histograms, otherwise weighted aggregation\n",
    "\n",
    "    Returns\n",
    "    ------- \n",
    "    D : ndarray \n",
    "        LBP descriptor\n",
    "\n",
    "    '''\n",
    "\n",
    "    #extract image patch\n",
    "    psize = patch_size // 2\n",
    "    x, y, z = point\n",
    "    patch = img[x - psize - int(math.ceil(ico_radius)) : x + psize + int(math.ceil(ico_radius)) + 1,\n",
    "                y - psize - int(math.ceil(ico_radius)) : y + psize + int(math.ceil(ico_radius)) + 1,\n",
    "                z - psize - int(math.ceil(ico_radius)) : z + psize + int(math.ceil(ico_radius)) + 1]\n",
    "    \n",
    "    patch_coords = cartesian((range(patch_size), range(patch_size), range(patch_size))) + 1    \n",
    "\n",
    "    #construct icosahedron for uniform sampling on sphere surface\n",
    "    ico = icosphere(subdivisions = ico_level, radius = ico_radius)\n",
    "    ico_coords = np.array(ico.vertices)\n",
    "    theta = np.arccos(ico_coords[:, 2] / ico_radius)\n",
    "    phi = np.arctan2(ico_coords[:, 1], ico_coords[:, 0])\n",
    "\n",
    "    #get Spherical Harmonics expansion coefficients (up to degree sph_degree)\n",
    "    m = list(itertools.chain.from_iterable([[i for i in range(-n,n+1)] for n in range(sph_degree)]))\n",
    "    m = np.array(m)\n",
    "\n",
    "    l = list(itertools.chain.from_iterable([[k for _ in range(2 * k + 1)] for k in range(sph_degree)]))\n",
    "    l = np.array(l)\n",
    "\n",
    "    Y = sph_harm(m[None, :], l[None, :], theta[:, None], phi[:, None])\n",
    "\n",
    "    #sample sphere neighbors for each voxel in patch and interpolate intensity\n",
    "    mapped_coords = patch_coords[None, :, :] + ico_coords[:, None, :]\n",
    "    mapped_int = map_coordinates(patch, mapped_coords.T, order = 3)\n",
    "    center_int = patch[ico_radius : -ico_radius, ico_radius : -ico_radius, ico_radius : -ico_radius]\n",
    "\n",
    "    #Compute kurtosis (for better rotation invariance)\n",
    "    kurt = kurtosis(mapped_int)\n",
    "\n",
    "    #Apply sign function and pass obtain spherical expansion coefficients for each sample\n",
    "    f = np.greater_equal(center_int.ravel()[:, None], mapped_int).astype('int')\n",
    "    c = f.dot(Y)\n",
    "\n",
    "    #obtain frequency components of threshold function by integrating and normalizing over orders m\n",
    "    f = np.multiply(c[:, None, l == 0], Y[None, :, l == 0])\n",
    "    for n in range(1, sph_degree):\n",
    "        f = np.concatenate((f, np.sum(np.multiply(c[:, None, l == n], Y[None, :, l == n]),\n",
    "                            axis=2, keepdims=True)), axis=2)\n",
    "    f = np.sqrt(np.sum(f**2, axis=1))\n",
    "\n",
    "    #keep real parts of decomposition and kurtosis\n",
    "    f = np.real(f)\n",
    "    kurt = np.real(kurt)\n",
    "\n",
    "    #extract histograms\n",
    "    H = np.histogram(kurt, bins = n_bins)[0]\n",
    "    for i in range(sph_degree):\n",
    "        H = np.column_stack((H, np.histogram(f[:, i], bins = n_bins)[0]))\n",
    "    H = normalize(H, axis = 0)\n",
    "\n",
    "    #Return Descriptor (concatenated or aggregated histograms)\n",
    "    if concatenate is True:\n",
    "        D = H.T.ravel()\n",
    "    else: \n",
    "        D = H.sum(axis = 1)\n",
    "    D = D / np.linalg.norm(D)\n",
    "\n",
    "    return D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def hog_3d_proj(point, image, psize = 5, rsize = 15, orientation = 'vertices', level = 1, mode = 'concatenated', \n",
    "                rot_inv = False, norm = 'l2'):\n",
    "    '''\n",
    "    Computes a 3D variant of the HOG Descriptor for an image region centered arounda voxel\n",
    "    The Region of size (rsize x rsize x rsize) is compartmentalized into a set of disjoint patches,\n",
    "    each of size (psize x psize x psize). A histogram of oriented gradients is computed for each patch, \n",
    "    with the orientation bins corresponding to vertices of centers of faces of a regular (refined) icosahedron.\n",
    "    The final descriptor is a weighted aggregate of those histograms. Currently, implementation supports regions arranged in\n",
    "    3x3x3 patches.\n",
    "\n",
    "    Reference: Alexander Klaser, Marcin Marszalek, Cordelia Schmid. \n",
    "               A Spatio-Temporal Descriptor Based on 3D-Gradients. \n",
    "               BMVC 2008 - 19th British Machine Vision Conference, Sep 2008, Leeds, United Kingdom.pp.275:1-10. \n",
    "               DOI:10.5244/C.22.99\n",
    "\n",
    "    ...\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point : array - like\n",
    "        the voxels to be characterized\n",
    "    image : ndarray\n",
    "        the image containing the voxels\n",
    "    psize : int\n",
    "        size of patches in region\n",
    "    rsize : int\n",
    "        size of region around central voxel\n",
    "    orientation : string\n",
    "        whether to associate histogram bins with vertices of centroids of faces of the icosahedron\n",
    "    ico_coords : string\n",
    "        coordinate system to define icosahedron on\n",
    "    level : int\n",
    "        number of refienement steps for icosahedron\n",
    "    mode : string\n",
    "        chooses whether to concatenate or aggregate patch histograms to form final descriptor\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    D : ndarray\n",
    "        voxel descriptor\n",
    "\n",
    "    '''\n",
    "\n",
    "    #sanity check\n",
    "    assert type(rsize // psize) == int, print(\"Wrong combination of regional and patch sizes\")\n",
    "\n",
    "    #set params\n",
    "    rs = rsize // 2\n",
    "    ps = psize // 2\n",
    "    ncells = rsize // psize\n",
    "    \n",
    "    # get icosahedron\n",
    "    ico = icosphere(subdivisions = level)\n",
    "    if orientation  == 'faces':\n",
    "        axes = np.array(ico.face_normals)\n",
    "    else:\n",
    "        axes = np.array(ico.vertices)\n",
    "\n",
    "    # get average masks\n",
    "    region_mask = getAverageMask(rsize // psize, 'manhattan')\n",
    "    patch_mask = getAverageMask(psize, 'manhattan')\n",
    "\n",
    "    #calculate partial derivatives\n",
    "    x, y, z = point \n",
    "    xp = range(- rs + ps, rs - ps + 1, psize)\n",
    "    yp = range(- rs + ps, rs - ps + 1, psize)\n",
    "    zp = range(- rs + ps, rs - ps + 1, psize)\n",
    "    patch_centers = cartesian((xp, yp, zp))\n",
    "    patch_locations = patch_centers + psize\n",
    "\n",
    "    # extracting +1 voxel in each direction for computational consistency\n",
    "    region = image[x - rs - 1 : x + rs + 2,\n",
    "                   y - rs - 1 : y + rs + 2,\n",
    "                   z - rs - 1 : z + rs + 2]\n",
    "        \n",
    "    i_dx, i_dy, i_dz = getGradients(region)\n",
    "\n",
    "    #get gradients at the patch level\n",
    "    dx = np.array([i_dx[ploc[0] : ploc[0] + psize, ploc[1] : ploc[1] + psize,\n",
    "                        ploc[2] : ploc[2] + psize] for ploc in patch_locations])\n",
    "\n",
    "    dy = np.array([i_dy[ploc[0] : ploc[0] + psize, ploc[1] : ploc[1] + psize,\n",
    "                        ploc[2] : ploc[2] + psize] for ploc in patch_locations])\n",
    "    \n",
    "    dz = np.array([i_dz[ploc[0] : ploc[0] + psize, ploc[1] : ploc[1] + psize,\n",
    "                        ploc[2] : ploc[2] + psize] for ploc in patch_locations])\n",
    "\n",
    "    dx = dx.reshape((ncells**3, psize**3))\n",
    "    dy = dy.reshape((ncells**3, psize**3))\n",
    "    dz = dz.reshape((ncells**3, psize**3))\n",
    "\n",
    "    #collect all gradients in one array and calculate magnitudes\n",
    "    raw_gradients = np.dstack((dx, dy, dz))\n",
    "    if rot_inv is True:\n",
    "        #rotate region according to dominant direction to achieve rotational invariance\n",
    "        R = structure_tensor_3D(raw_gradients, getAverageMask(rsize, 'gaussian'))\n",
    "        gradients = R.T.dot(raw_gradients.reshape((-1, 3)).T) \n",
    "        gradients = gradients.reshape(3, raw_gradients.shape[1], raw_gradients.shape[0]).T\n",
    "    else:\n",
    "        gradients = raw_gradients\n",
    "    gradient_magnitudes = np.linalg.norm(gradients, axis = 2)\n",
    "\n",
    "    #project gradients to icosahedron orientation axes\n",
    "    projected_gradients = gradients.dot(axes.T)\n",
    "    projected_gradients /= (gradient_magnitudes[:, :, None]+0.001)\n",
    "\n",
    "    # compute theshold to clip projected gradients and recalculate magnitude\n",
    "    inner_prods = linear_kernel(axes)[0, :]\n",
    "    thres = np.sort(inner_prods)[-2]\n",
    "\n",
    "    projected_gradients -= thres\n",
    "    projected_gradients[projected_gradients < 0] = 0\n",
    "    projected_gradient_magnitudes = np.linalg.norm(projected_gradients, axis = 2)\n",
    "\n",
    "    #distribute original magnitude in orientation bins\n",
    "    gradient_histograms = projected_gradients * (gradient_magnitudes[:, :, None] / (projected_gradient_magnitudes[:, :, None]+0.001))\n",
    "    D = gradient_histograms.sum(axis = 1)\n",
    "\n",
    "    if mode == 'flatten':\n",
    "        Descriptor = (region_mask.ravel()[:, None] * D).ravel()\n",
    "    else:\n",
    "        Descriptor = region_mask.ravel().dot(D)\n",
    "\n",
    "    if norm == 'l2':\n",
    "        Descriptor = Descriptor / np.linalg.norm(Descriptor)\n",
    "    if norm == 'l2-hys':\n",
    "        Descriptor = Descriptor / np.linalg.norm(Descriptor)\n",
    "        Descriptor = np.clip(Descriptor, a_min = 0, a_max = 0.25)\n",
    "        Descriptor = Descriptor / np.linalg.norm(Descriptor)\n",
    "    \n",
    "    return Descriptor\n",
    "   \n",
    "\n",
    "def fn_hog_lbp_descriptor(coordinate, array):  \n",
    "    descriptor_lbp = lbp_ri_sh(coordinate, array, 5, 4, 2, 2, concatenate = True)\n",
    "    descriptor_hog = hog_3d_proj(coordinate, array) \n",
    "    descriptor_lbp_hog = np.concatenate([descriptor_hog,descriptor_lbp])\n",
    "    return descriptor_lbp_hog.reshape(1, len(descriptor_lbp_hog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODEWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cw_coordinate_sampling(segm, roi):\n",
    "    coord_labels = []\n",
    "    coordinates = []\n",
    "    \n",
    "    mask_values = segm[roi_perimeter[:,0], roi_perimeter[:,1], roi_perimeter[:,2]]\n",
    "\n",
    "    threshold_4 = np.count_nonzero(mask_values == 4) #.shape[0]#len(segm[segm[coordinate_cube]==4])\n",
    "    \n",
    "    threshold_1=int(threshold_4/1.3)\n",
    "    threshold_2=int(threshold_4*1.3)\n",
    "    threshold_3=int(threshold_4/1.3)\n",
    "    threshold_0=int(threshold_4/1.4)\n",
    "   \n",
    "    i_0,i_1,i_2,i_3,i_4 =0, 0, 0, 0, 0\n",
    "    \n",
    "        \n",
    "        \n",
    "    for coordinate in roi_perimeter:\n",
    "        x,y,z = coordinate\n",
    "        if roi[x,y,z]==1:\n",
    "            if segm[x,y,z]==0 and i_0<threshold_0:\n",
    "                coordinates.append(coordinate)\n",
    "                i_0+=1\n",
    "                descriptor = fn_hog_lbp_descriptor(coordinate, mri_scan)\n",
    "                descriptor = np.reshape(descriptor, [descriptor.shape[1],])\n",
    "                all_descriptors.append(descriptor)    \n",
    "            elif segm[x,y,z]==1 and i_1<threshold_1:\n",
    "                coordinates.append(coordinate)\n",
    "                i_1+=1\n",
    "                descriptor = fn_hog_lbp_descriptor(coordinate, mri_scan)\n",
    "                descriptor = np.reshape(descriptor, [descriptor.shape[1],])\n",
    "                all_descriptors.append(descriptor)    \n",
    "            elif segm[x,y,z]==2 and i_2<threshold_2:\n",
    "                coordinates.append(coordinate)\n",
    "                i_2+=1\n",
    "                descriptor = fn_hog_lbp_descriptor(coordinate, mri_scan)\n",
    "                descriptor = np.reshape(descriptor, [descriptor.shape[1],])\n",
    "                all_descriptors.append(descriptor)    \n",
    "            elif segm[x,y,z]==3 and i_3<threshold_3:\n",
    "                coordinates.append(coordinate)\n",
    "                i_3+=1\n",
    "                descriptor = fn_hog_lbp_descriptor(coordinate, mri_scan)\n",
    "                descriptor = np.reshape(descriptor, [descriptor.shape[1],])\n",
    "                all_descriptors.append(descriptor)    \n",
    "            elif segm[x,y,z]==4 and i_4<threshold_4:\n",
    "                coordinates.append(coordinate)\n",
    "                i_4+=1 \n",
    "                descriptor = fn_hog_lbp_descriptor(coordinate, mri_scan)\n",
    "                descriptor = np.reshape(descriptor, [descriptor.shape[1],])\n",
    "                all_descriptors.append(descriptor)    \n",
    "                \n",
    "\n",
    "    # for value, count in zip(unique_values, counts):\n",
    "    #     print(f\"{value}: {count}\")\n",
    "    coordinates = np.array(coordinates)  \n",
    "    np.random.shuffle(coordinates)      \n",
    "    return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9001104\n",
      "got coordinates for scan no 9001104\n",
      "9002430\n",
      "got coordinates for scan no 9002430\n",
      "9002817\n",
      "got coordinates for scan no 9002817\n",
      "9003430\n",
      "got coordinates for scan no 9003430\n",
      "9004175\n",
      "got coordinates for scan no 9004175\n",
      "9005075\n",
      "got coordinates for scan no 9005075\n",
      "9005132\n",
      "got coordinates for scan no 9005132\n",
      "9006723\n",
      "got coordinates for scan no 9006723\n",
      "9007827\n",
      "9008561\n",
      "9011115\n",
      "got coordinates for scan no 9011115\n",
      "9011420\n",
      "9012867\n",
      "9013161\n",
      "9017909\n",
      "got coordinates for scan no 9017909\n",
      "9019287\n",
      "got coordinates for scan no 9019287\n",
      "9023193\n",
      "got coordinates for scan no 9023193\n",
      "9027422\n",
      "9031426\n",
      "9033937\n",
      "got coordinates for scan no 9033937\n",
      "9034644\n",
      "got coordinates for scan no 9034644\n",
      "9036287\n",
      "got coordinates for scan no 9036287\n",
      "9036770\n",
      "got coordinates for scan no 9036770\n",
      "9036948\n",
      "got coordinates for scan no 9036948\n",
      "9039627\n",
      "9040944\n",
      "got coordinates for scan no 9040944\n",
      "9041946\n",
      "got coordinates for scan no 9041946\n",
      "9044005\n",
      "9047539\n",
      "got coordinates for scan no 9047539\n",
      "9052335\n",
      "got coordinates for scan no 9052335\n",
      "9052956\n",
      "9053047\n",
      "9055836\n",
      "9056326\n",
      "9057417\n",
      "9065272\n",
      "9069761\n",
      "got coordinates for scan no 9069761\n",
      "9073948\n",
      "got coordinates for scan no 9073948\n",
      "9074437\n",
      "9075815\n",
      "9080864\n",
      "got coordinates for scan no 9080864\n",
      "9081306\n",
      "9083500\n",
      "got coordinates for scan no 9083500\n",
      "9089627\n",
      "got coordinates for scan no 9089627\n",
      "9090290\n",
      "got coordinates for scan no 9090290\n",
      "9093622\n",
      "got coordinates for scan no 9093622\n"
     ]
    }
   ],
   "source": [
    "for scan_no in os.listdir('data'):\n",
    "    try:\n",
    "        print(scan_no)\n",
    "        mri_scan = fn_scan_to_array('data/' + scan_no)\n",
    "        segm_mask = fn_segm_mask_to_array(scan_no)\n",
    "        roi = np.load('data/'+ scan_no+ '/roi.npy')\n",
    "        coordinates = cw_coordinate_sampling(segm_mask, roi)\n",
    "        print('got coordinates for scan no' , scan_no)\n",
    "        np.save('all_descriptors.npy', all_descriptors)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1013.7391304347826\n"
     ]
    }
   ],
   "source": [
    "k = len(os.listdir(\"data\"))\n",
    "\n",
    "print(np.load(\"all_descriptors.npy\").shape[0] / k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_codewords(local_descriptors, no_of_words):\n",
    "    random_state=42\n",
    "    kmeans_model=KMeans(n_clusters=no_of_words, verbose=False, init='k-means++', random_state=random_state)\n",
    "    kmeans_model.fit(local_descriptors)\n",
    "    return kmeans_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_descriptors = np.array(all_descriptors)\n",
    "np.save('all_descriptors.npy', all_descriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  1.91952094e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -3.04163061e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.95000876e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.08101642e-01,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.85031697e-02, -6.08567310e-03,  4.31336692e-03,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  7.19988154e-09,  0.00000000e+00,\n",
       "        1.14819179e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        2.40337451e-02, -2.76542166e-08,  1.19314743e-03,  1.36857220e-02,\n",
       "       -5.60597150e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -4.05641340e-08,  0.00000000e+00,  2.15605089e-02,  9.42096335e-02,\n",
       "        1.39259416e-01,  2.59231188e-02, -1.66928178e-02, -1.10397370e-01,\n",
       "       -2.51654970e-02, -1.85227800e-01, -7.21440434e-02, -2.95281069e-02,\n",
       "        3.24670336e-02,  8.72521978e-03, -6.03883366e-03,  4.94123920e-02,\n",
       "        1.07044923e-01,  2.52540588e-04,  4.36260989e-03,  2.37418138e-02,\n",
       "        1.17698907e-01,  3.00331881e-02,  4.28684771e-02,  5.13411563e-02,\n",
       "       -6.29137425e-03,  1.50165940e-02,  2.18130494e-03,  2.78518831e-02,\n",
       "        2.18130494e-03,  1.50165940e-02,  2.18130494e-03, -8.47267919e-03,\n",
       "        1.82760130e-02,  1.11353034e-03,  7.08769912e-02,  1.11353034e-03,\n",
       "       -3.37682001e-02,  1.67029551e-03,  1.67029551e-03, -1.54921871e-02,\n",
       "        1.07429017e-01,  2.78382584e-03, -3.20979046e-02,  1.99463085e-02,\n",
       "       -1.54921871e-02,  5.42712737e-02, -3.20979046e-02, -1.49354220e-02,\n",
       "        2.05030736e-02, -1.60489523e-02, -8.46988828e-02,  2.05030736e-02,\n",
       "        9.13800648e-02, -6.64228698e-02, -1.32651264e-02,  1.93895433e-02,\n",
       "       -4.75900917e-02,  1.93895433e-02,  1.93895433e-02,  1.88327781e-02,\n",
       "        1.67029551e-03, -4.87036220e-02,  7.13194069e-04,  1.75323924e-02,\n",
       "        7.13194069e-04, -1.61060042e-02,  3.36383966e-02,  1.89587805e-02,\n",
       "        1.82455864e-02, -1.53928102e-02,  8.48091856e-02,  1.96719746e-02,\n",
       "       -3.07856203e-02,  2.13958221e-03,  1.89587805e-02, -3.07856203e-02,\n",
       "        3.64911729e-02,  3.50647847e-02, -3.00724263e-02, -1.39664220e-02,\n",
       "        3.79175610e-02, -7.76772449e-02, -7.69640508e-02, -2.93592322e-02,\n",
       "       -6.22844347e-02, -5.87184644e-02,  5.54499534e-02,  3.86307551e-02,\n",
       "        1.96719746e-02,  2.10983627e-02, -3.00724263e-02,  1.01628384e-01,\n",
       "       -3.40851088e-02,  3.41409859e-04,  5.30054174e-02,  5.26640075e-02,\n",
       "        1.36563944e-03, -3.30608792e-02,  1.02422958e-03,  3.61335679e-02,\n",
       "        5.36882371e-02, -1.61890297e-02, -6.85116274e-02,  1.36563944e-03,\n",
       "       -1.02596736e-01, -6.81702175e-02,  1.36563944e-03,  1.92617185e-02,\n",
       "        2.04845915e-03, -1.51648002e-02,  3.64749778e-02,  7.05600866e-02,\n",
       "        7.15843161e-02,  1.70704930e-03,  3.71577975e-02,  3.54507482e-02,\n",
       "        7.09014964e-02, -5.09569582e-02,  5.43710568e-02,  5.33468272e-02,\n",
       "       -1.02938146e-01, -1.19468586e-01,  2.24890656e-05, -1.64732329e-02,\n",
       "       -3.29914440e-02,  6.60953332e-02, -3.29464658e-02,  4.95771222e-02,\n",
       "        8.99562625e-05,  1.65631892e-02,  4.95996113e-02,  1.65631892e-02,\n",
       "       -3.29464658e-02, -1.64057657e-02,  1.65856782e-02,  3.31038893e-02,\n",
       "       -1.64957220e-02, -4.93747206e-02,  4.49781313e-05,  1.15672455e-01,\n",
       "       -1.15380098e-01, -6.58929316e-02,  1.15717434e-01, -4.93297425e-02,\n",
       "        1.66981236e-02, -3.29014877e-02,  1.66306564e-02,  6.61628004e-02,\n",
       "        1.34934394e-04, -3.29014877e-02, -1.64507438e-02, -6.59603988e-02])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_descriptors[0] - all_descriptors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\MySoftware\\python3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "codewords = find_codewords(all_descriptors, 40)\n",
    "np.save('codewords.npy', codewords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 192)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codewords.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28 18\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "i,j=0,0\n",
    "for scan_no in os.listdir('data'):\n",
    "    try:\n",
    "        mri_scan = fn_scan_to_array('data/' + scan_no)\n",
    "        i+=1\n",
    "    except Exception:\n",
    "        j+=1\n",
    "        \n",
    "print(i,j)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
